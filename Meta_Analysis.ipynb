{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random-effects meta-analysis result:\n",
      "Combined OR: 2.34 [1.84; 2.96]\n",
      "I²: 99.97%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "data = {\n",
    "    \"Study\": [\"Reece (2007)\", \"Rooban et al. (2008)\", \"Morio et al. (2008)\", \"Gupta et al. (2012)\", \"Nives Protrka et al. (2013)\", \"Rommel et al. (2016)\", \"Shetty et al. (2016)\", \n",
    "              \"Aguilar-Zinser et al. (2008)\", \"Badel et al. (2014)\", \"Tanner et al. (2014)\", \"Tanner et al. (2015)\", \"Sharma et al. (2018)\"],\n",
    "    \"OR\": [1.97, 1.69, 7.13, 1.92, 24.10, 4.80, 4.06, \n",
    "           1.04, 1.04, 1.57, 1.85, 1.67],\n",
    "    \"Lower CI\": [1.10, 1.01, 3.67, 1.21, 13.18, 2.80, 2.22, \n",
    "                 0.73, 0.64, 1.66, 2.05, 0.34],\n",
    "    \"Upper CI\": [3.54, 2.83, 13.82, 3.04, 44.08, 8.20, 7.44, \n",
    "                 1.23, 1.04, 2.10, 3.15, 1.16],\n",
    "    \"Sample Size\": [233, 100, 18, 126, 200, 200, 571, \n",
    "                    629, 505, 612, 630, 200]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate LogOR and Standard Error (SE)\n",
    "df[\"LogOR\"] = np.log(df[\"OR\"])\n",
    "df[\"SE\"] = (np.log(df[\"Upper CI\"]) - np.log(df[\"Lower CI\"])) / 3.92\n",
    "\n",
    "# Weight calculation (inverse of variance as before)\n",
    "df[\"Weight\"] = df[\"Sample Size\"] / (df[\"SE\"] ** 2)\n",
    "\n",
    "# Calculate the fixed-effect pooled estimate (LogOR and SE)\n",
    "combined_logOR = np.sum(df[\"LogOR\"] * df[\"Weight\"]) / np.sum(df[\"Weight\"])\n",
    "combined_SE = np.sqrt(1 / np.sum(df[\"Weight\"]))  # SE of the combined effect\n",
    "\n",
    "# Calculate Q-statistic (Cochran's Q) and degrees of freedom\n",
    "Q = np.sum(df[\"Weight\"] * (df[\"LogOR\"] - combined_logOR) ** 2)\n",
    "df_ = len(df) - 1  # Degrees of freedom\n",
    "\n",
    "# Estimate between-study variance (tau^2) using DerSimonian-Laird method\n",
    "tau2 = max((Q - df_) / np.sum(df[\"Weight\"]), 0)\n",
    "\n",
    "# Adjust weights for the random-effects model\n",
    "df[\"Weight_random\"] = 1 / (df[\"SE\"]**2 + tau2)\n",
    "\n",
    "# Calculate the new pooled estimate using random-effects weights\n",
    "combined_logOR_random = np.sum(df[\"LogOR\"] * df[\"Weight_random\"]) / np.sum(df[\"Weight_random\"])\n",
    "combined_SE_random = np.sqrt(1 / np.sum(df[\"Weight_random\"]))  # SE of the combined effect for random-effects model\n",
    "\n",
    "# Calculate 95% CI for the combined effect using random-effects model\n",
    "lower_CI_combined_random = np.exp(combined_logOR_random - 1.96 * combined_SE_random)\n",
    "upper_CI_combined_random = np.exp(combined_logOR_random + 1.96 * combined_SE_random)\n",
    "\n",
    "# Calculate I² for random-effects model\n",
    "I2_random = 100 * (Q - df_) / Q if Q > df_ else 0\n",
    "\n",
    "# Output results for random-effects model\n",
    "print(\"Random-effects meta-analysis result:\")\n",
    "print(f\"Combined OR: {np.exp(combined_logOR_random):.2f} [{lower_CI_combined_random:.2f}; {upper_CI_combined_random:.2f}]\")\n",
    "print(f\"I²: {I2_random:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After excluding Reece (2007):\n",
      "Combined OR: 2.37 [1.85; 3.03]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Rooban et al. (2008):\n",
      "Combined OR: 2.40 [1.87; 3.07]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Morio et al. (2008):\n",
      "Combined OR: 2.17 [1.70; 2.77]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Gupta et al. (2012):\n",
      "Combined OR: 2.38 [1.86; 3.05]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Nives Protrka et al. (2013):\n",
      "Combined OR: 1.90 [1.55; 2.33]\n",
      "I²: 99.95%\n",
      "--------------------------------------------------\n",
      "After excluding Rommel et al. (2016):\n",
      "Combined OR: 2.19 [1.72; 2.78]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Shetty et al. (2016):\n",
      "Combined OR: 2.22 [1.76; 2.80]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Aguilar-Zinser et al. (2008):\n",
      "Combined OR: 2.55 [2.00; 3.26]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Badel et al. (2014):\n",
      "Combined OR: 2.56 [2.00; 3.27]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Tanner et al. (2014):\n",
      "Combined OR: 2.60 [1.84; 3.65]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Tanner et al. (2015):\n",
      "Combined OR: 2.43 [1.87; 3.16]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Sharma et al. (2018):\n",
      "Combined OR: 2.39 [1.87; 3.06]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "Summary of Sensitivity Analysis:\n",
      "Overall Combined ORs: [2.3662605454735273, 2.3998377667109683, 2.1685490801759317, 2.377956204002011, 1.9032215400875583, 2.1883790358672117, 2.2167454810964826, 2.5529959259052957, 2.559395994124354, 2.59595660607857, 2.430996377661944, 2.393308902246722]\n",
      "Overall I² values: [99.97361351268142, 99.9737105608106, 99.97346259026584, 99.97364571717281, 99.95470682654047, 99.97111938525127, 99.9691263980429, 99.96838102282719, 99.96889602571304, 99.97368252025765, 99.97229780164918, 99.97371022659026]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store results\n",
    "combined_ORs = []\n",
    "I2_values = []\n",
    "\n",
    "# Sensitivity Analysis: Exclude each study one by one\n",
    "for index, study in df.iterrows():\n",
    "    df_sensitivity = df.drop(index)  # Remove one study at a time\n",
    "    \n",
    "    # Calculate LogOR and SE for the remaining studies\n",
    "    logOR_sensitivity = np.log(df_sensitivity[\"OR\"])\n",
    "    SE_sensitivity = (np.log(df_sensitivity[\"Upper CI\"]) - np.log(df_sensitivity[\"Lower CI\"])) / 3.92\n",
    "    weights_sensitivity = df_sensitivity[\"Sample Size\"] / (SE_sensitivity ** 2)\n",
    "\n",
    "    # Random effects meta-analysis for the remaining studies\n",
    "    combined_logOR_sensitivity = np.sum(logOR_sensitivity * weights_sensitivity) / np.sum(weights_sensitivity)\n",
    "    combined_SE_sensitivity = np.sqrt(1 / np.sum(weights_sensitivity))  # SE for the combined effect\n",
    "    Q_sensitivity = np.sum(weights_sensitivity * (logOR_sensitivity - combined_logOR_sensitivity) ** 2)\n",
    "    df_ = len(df_sensitivity) - 1  # Degrees of freedom for the remaining studies\n",
    "    tau2_sensitivity = max((Q_sensitivity - df_) / np.sum(weights_sensitivity), 0)\n",
    "    \n",
    "    # Recalculate weights for random effects\n",
    "    df_sensitivity[\"Weight_random\"] = 1 / (SE_sensitivity**2 + tau2_sensitivity)\n",
    "    \n",
    "    # Combined OR for random effects\n",
    "    combined_logOR_random_sensitivity = np.sum(logOR_sensitivity * df_sensitivity[\"Weight_random\"]) / np.sum(df_sensitivity[\"Weight_random\"])\n",
    "    combined_SE_random_sensitivity = np.sqrt(1 / np.sum(df_sensitivity[\"Weight_random\"]))  # SE for random effects\n",
    "    \n",
    "    # Confidence intervals for combined OR\n",
    "    lower_CI_combined_random_sensitivity = np.exp(combined_logOR_random_sensitivity - 1.96 * combined_SE_random_sensitivity)\n",
    "    upper_CI_combined_random_sensitivity = np.exp(combined_logOR_random_sensitivity + 1.96 * combined_SE_random_sensitivity)\n",
    "    \n",
    "    # Calculate I²\n",
    "    I2_sensitivity = 100 * (Q_sensitivity - df_) / Q_sensitivity if Q_sensitivity > df_ else 0\n",
    "    \n",
    "    # Append results for analysis\n",
    "    combined_ORs.append(np.exp(combined_logOR_random_sensitivity))\n",
    "    I2_values.append(I2_sensitivity)\n",
    "    \n",
    "    # Output the result for this iteration\n",
    "    print(f\"After excluding {study['Study']}:\")\n",
    "    print(f\"Combined OR: {np.exp(combined_logOR_random_sensitivity):.2f} [{lower_CI_combined_random_sensitivity:.2f}; {upper_CI_combined_random_sensitivity:.2f}]\")\n",
    "    print(f\"I²: {I2_sensitivity:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Compare the final combined OR and I² from all iterations\n",
    "print(\"Summary of Sensitivity Analysis:\")\n",
    "print(f\"Overall Combined ORs: {combined_ORs}\")\n",
    "print(f\"Overall I² values: {I2_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "smoking            0      1\n",
      "dental caries              \n",
      "0              20207  10418\n",
      "1               4459   3900\n",
      "Odds Ratio (OR): 1.70\n",
      "95% Confidence Interval: [1.62, 1.78]\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/rishav/Downloads/archive/train_dataset.csv\")\n",
    "# Create a contingency table for dental caries and smoking\n",
    "contingency = pd.crosstab(df[\"dental caries\"], df[\"smoking\"])\n",
    "\n",
    "# Print the table\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency)\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Extract values from the contingency table\n",
    "a = contingency.loc[1, 1] if 1 in contingency.columns else 0  # Smokers with caries\n",
    "b = contingency.loc[1, 0] if 0 in contingency.columns else 0  # Non-smokers with caries\n",
    "c = contingency.loc[0, 1] if 1 in contingency.columns else 0  # Smokers without caries\n",
    "d = contingency.loc[0, 0] if 0 in contingency.columns else 0  # Non-smokers without caries\n",
    "\n",
    "# Calculate Odds Ratio\n",
    "odds_ratio = (a * d) / (b * c) if b * c > 0 else np.inf\n",
    "\n",
    "# Calculate the standard error of log(OR)\n",
    "if all(x > 0 for x in [a, b, c, d]):  # Check to avoid division by zero\n",
    "    se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "    log_or = np.log(odds_ratio)\n",
    "    ci_lower = np.exp(log_or - 1.96 * se_log_or)\n",
    "    ci_upper = np.exp(log_or + 1.96 * se_log_or)\n",
    "else:\n",
    "    ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "# Fisher's Exact Test for p-value\n",
    "_, p_value = fisher_exact([[a, b], [c, d]])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Odds Ratio (OR): {odds_ratio:.2f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"P-value: {p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power for Meta-Analysis (Random-effects): 1.0000\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate the z-statistic for the combined LogOR\n",
    "z_combined = combined_logOR_random / combined_SE_random\n",
    "\n",
    "# Calculate the p-value corresponding to the z-statistic\n",
    "p_value_combined = 2 * (1 - stats.norm.cdf(abs(z_combined)))\n",
    "\n",
    "# Calculate the power for detecting a significant effect using normal distribution\n",
    "# Power = 1 - Beta, where Beta is the probability of a Type II error\n",
    "# Calculate the critical z-value for alpha = 0.05 (two-tailed)\n",
    "z_alpha = stats.norm.ppf(0.975)  # for 95% CI\n",
    "\n",
    "# Calculate the power\n",
    "power_combined = 1 - stats.norm.cdf(z_alpha - abs(z_combined))\n",
    "print(f\"Power for Meta-Analysis (Random-effects): {power_combined:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Power for Fisher's Exact Test (Contingency Table): 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Function to simulate the contingency table and compute power\n",
    "def simulate_fisher_power(odds_ratio, sample_size, n_simulations=1000, alpha=0.05):\n",
    "    power_count = 0\n",
    "    \n",
    "    # Simulate data for each trial\n",
    "    for _ in range(n_simulations):\n",
    "        # Simulate the values a, b, c, and d based on the given odds ratio\n",
    "        a = np.random.binomial(sample_size // 2, odds_ratio / (1 + odds_ratio))\n",
    "        b = sample_size // 2 - a\n",
    "        c = np.random.binomial(sample_size // 2, 1 / (1 + odds_ratio))\n",
    "        d = sample_size // 2 - c\n",
    "        \n",
    "        # Construct the contingency table\n",
    "        contingency_table = np.array([[a, b], [c, d]])\n",
    "        \n",
    "        # Perform Fisher's Exact Test\n",
    "        _, p_value = fisher_exact(contingency_table)\n",
    "        \n",
    "        # Check if we reject the null hypothesis (p-value < alpha)\n",
    "        if p_value < alpha:\n",
    "            power_count += 1\n",
    "    \n",
    "    # Estimate the power\n",
    "    power = power_count / n_simulations\n",
    "    return power\n",
    "\n",
    "# Set the odds ratio and sample size\n",
    "odds_ratio_sample = odds_ratio  # use the calculated odds ratio\n",
    "sample_size = contingency.sum().sum()\n",
    "\n",
    "# Run the simulation to estimate the power\n",
    "power_fisher_simulated = simulate_fisher_power(odds_ratio_sample, sample_size)\n",
    "\n",
    "# Print the simulated power\n",
    "print(f\"Simulated Power for Fisher's Exact Test (Contingency Table): {power_fisher_simulated:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "smoking            0      1\n",
      "dental caries              \n",
      "0              20207  10418\n",
      "1               4459   3900\n",
      "Odds Ratio (OR): 1.70\n",
      "95% Confidence Interval: [1.62, 1.78]\n",
      "P-value: 0.000\n",
      "Power for Fisher's Exact Test: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/rishav/Downloads/archive/train_dataset.csv\")\n",
    "\n",
    "# Create a contingency table for dental caries and smoking\n",
    "contingency = pd.crosstab(df[\"dental caries\"], df[\"smoking\"])\n",
    "\n",
    "# Print the table\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency)\n",
    "\n",
    "# Extract values from the contingency table\n",
    "a = contingency.loc[1, 1] if 1 in contingency.columns else 0  # Smokers with caries\n",
    "b = contingency.loc[1, 0] if 0 in contingency.columns else 0  # Non-smokers with caries\n",
    "c = contingency.loc[0, 1] if 1 in contingency.columns else 0  # Smokers without caries\n",
    "d = contingency.loc[0, 0] if 0 in contingency.columns else 0  # Non-smokers without caries\n",
    "\n",
    "# Calculate Odds Ratio\n",
    "odds_ratio = (a * d) / (b * c) if b * c > 0 else np.inf\n",
    "\n",
    "# Calculate the standard error of log(OR)\n",
    "if all(x > 0 for x in [a, b, c, d]):  # Check to avoid division by zero\n",
    "    se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "    log_or = np.log(odds_ratio)\n",
    "    ci_lower = np.exp(log_or - 1.96 * se_log_or)\n",
    "    ci_upper = np.exp(log_or + 1.96 * se_log_or)\n",
    "else:\n",
    "    ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "# Fisher's Exact Test for p-value\n",
    "_, p_value = fisher_exact([[a, b], [c, d]])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Odds Ratio (OR): {odds_ratio:.2f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "# Power calculation for Fisher's Exact Test using the statsmodels approximation\n",
    "# You can adjust the odds_ratio for a more accurate analysis\n",
    "sample_size = contingency.sum().sum()  # Total number of observations\n",
    "effect_size = np.log(odds_ratio)  # Log of the odds ratio for effect size\n",
    "alpha = 0.05  # significance level\n",
    "\n",
    "# Assuming a 2x2 contingency table\n",
    "power_analysis = sms.GofChisquarePower()  # Use chi-square approximation for 2x2 table\n",
    "power_fisher = power_analysis.solve_power(effect_size=effect_size, nobs=sample_size, alpha=alpha)\n",
    "\n",
    "# Print the power of Fisher's Exact Test\n",
    "print(f\"Power for Fisher's Exact Test: {power_fisher:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table (200 Sampled Observations):\n",
      "smoking         0   1\n",
      "dental caries        \n",
      "0              54  26\n",
      "1              11   9\n",
      "Odds Ratio (OR): 1.70\n",
      "95% Confidence Interval: [0.63, 4.61]\n",
      "P-value: 0.307\n",
      "Power for Fisher's Exact Test: 0.9996\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/rishav/Downloads/archive/train_dataset.csv\")\n",
    "\n",
    "# Randomly sample 200 rows from the dataframe\n",
    "df_sample = df.sample(n=100, random_state=42)\n",
    "\n",
    "# Create a contingency table for dental caries and smoking\n",
    "contingency = pd.crosstab(df_sample[\"dental caries\"], df_sample[\"smoking\"])\n",
    "\n",
    "# Print the table\n",
    "print(\"Contingency Table (200 Sampled Observations):\")\n",
    "print(contingency)\n",
    "\n",
    "# Extract values from the contingency table\n",
    "a = contingency.loc[1, 1] if 1 in contingency.columns else 0  # Smokers with caries\n",
    "b = contingency.loc[1, 0] if 0 in contingency.columns else 0  # Non-smokers with caries\n",
    "c = contingency.loc[0, 1] if 1 in contingency.columns else 0  # Smokers without caries\n",
    "d = contingency.loc[0, 0] if 0 in contingency.columns else 0  # Non-smokers without caries\n",
    "\n",
    "# Calculate Odds Ratio\n",
    "odds_ratio = (a * d) / (b * c) if b * c > 0 else np.inf\n",
    "\n",
    "# Calculate the standard error of log(OR)\n",
    "if all(x > 0 for x in [a, b, c, d]):  # Check to avoid division by zero\n",
    "    se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "    log_or = np.log(odds_ratio)\n",
    "    ci_lower = np.exp(log_or - 1.96 * se_log_or)\n",
    "    ci_upper = np.exp(log_or + 1.96 * se_log_or)\n",
    "else:\n",
    "    ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "# Fisher's Exact Test for p-value\n",
    "_, p_value = fisher_exact([[a, b], [c, d]])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Odds Ratio (OR): {odds_ratio:.2f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "# Power calculation for Fisher's Exact Test using the statsmodels approximation\n",
    "# You can adjust the odds_ratio for a more accurate analysis\n",
    "sample_size = contingency.sum().sum()  # Total number of observations\n",
    "effect_size = np.log(odds_ratio)  # Log of the odds ratio for effect size\n",
    "alpha = 0.05  # significance level\n",
    "\n",
    "# Assuming a 2x2 contingency table\n",
    "power_analysis = sms.GofChisquarePower()  # Use chi-square approximation for 2x2 table\n",
    "power_fisher = power_analysis.solve_power(effect_size=effect_size, nobs=sample_size, alpha=alpha)\n",
    "\n",
    "# Print the power of Fisher's Exact Test\n",
    "print(f\"Power for Fisher's Exact Test: {power_fisher:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
