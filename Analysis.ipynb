{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-effect meta-analysis result:\n",
      "Combined OR: 1.55 [1.55; 1.56]\n",
      "I²: 99.97%\n",
      "\n",
      "Random-effects meta-analysis result:\n",
      "Combined OR: 2.34 [1.84; 2.96]\n",
      "I²: 99.97%\n",
      "\n",
      "Sensitivity Analysis:\n",
      "After excluding Reece (2007):\n",
      "Combined OR: 2.37 [1.85; 3.03]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Rooban et al. (2008):\n",
      "Combined OR: 2.40 [1.87; 3.07]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Morio et al. (2008):\n",
      "Combined OR: 2.17 [1.70; 2.77]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Gupta et al. (2012):\n",
      "Combined OR: 2.38 [1.86; 3.05]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Nives Protrka et al. (2013):\n",
      "Combined OR: 1.90 [1.55; 2.33]\n",
      "I²: 99.95%\n",
      "--------------------------------------------------\n",
      "After excluding Rommel et al. (2016):\n",
      "Combined OR: 2.19 [1.72; 2.78]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Shetty et al. (2016):\n",
      "Combined OR: 2.22 [1.76; 2.80]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Aguilar-Zinser et al. (2008):\n",
      "Combined OR: 2.55 [2.00; 3.26]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Badel et al. (2014):\n",
      "Combined OR: 2.56 [2.00; 3.27]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Tanner et al. (2014):\n",
      "Combined OR: 2.60 [1.84; 3.65]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Tanner et al. (2015):\n",
      "Combined OR: 2.43 [1.87; 3.16]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "After excluding Sharma et al. (2018):\n",
      "Combined OR: 2.39 [1.87; 3.06]\n",
      "I²: 99.97%\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary of Sensitivity Analysis:\n",
      "Combined ORs after excluding each study: [2.3662605454735273, 2.3998377667109683, 2.1685490801759317, 2.377956204002011, 1.9032215400875583, 2.1883790358672117, 2.2167454810964826, 2.5529959259052957, 2.559395994124354, 2.59595660607857, 2.430996377661944, 2.393308902246722]\n",
      "I² values after excluding each study: [99.97361351268142, 99.9737105608106, 99.97346259026584, 99.97364571717281, 99.95470682654047, 99.97111938525127, 99.9691263980429, 99.96838102282719, 99.96889602571304, 99.97368252025765, 99.97229780164918, 99.97371022659026]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "    \"Study\": [\"Reece (2007)\", \"Rooban et al. (2008)\", \"Morio et al. (2008)\", \"Gupta et al. (2012)\", \"Nives Protrka et al. (2013)\", \"Rommel et al. (2016)\", \"Shetty et al. (2016)\", \n",
    "              \"Aguilar-Zinser et al. (2008)\", \"Badel et al. (2014)\", \"Tanner et al. (2014)\", \"Tanner et al. (2015)\", \"Sharma et al. (2018)\"],\n",
    "    \"OR\": [1.97, 1.69, 7.13, 1.92, 24.10, 4.80, 4.06, \n",
    "           1.04, 1.04, 1.57, 1.85, 1.67],\n",
    "    \"Lower CI\": [1.10, 1.01, 3.67, 1.21, 13.18, 2.80, 2.22, \n",
    "                 0.73, 0.64, 1.66, 2.05, 0.34],\n",
    "    \"Upper CI\": [3.54, 2.83, 13.82, 3.04, 44.08, 8.20, 7.44, \n",
    "                 1.23, 1.04, 2.10, 3.15, 1.16],\n",
    "    \"Sample Size\": [233, 100, 18, 126, 200, 200, 571, \n",
    "                    629, 505, 612, 630, 200]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate LogOR and Standard Error (SE)\n",
    "df[\"LogOR\"] = np.log(df[\"OR\"])\n",
    "df[\"SE\"] = (np.log(df[\"Upper CI\"]) - np.log(df[\"Lower CI\"])) / 3.92\n",
    "\n",
    "# Adjust weights using sample size\n",
    "df[\"Weight\"] = df[\"Sample Size\"] / (df[\"SE\"] ** 2)\n",
    "\n",
    "# Calculate the combined effect (fixed-effect model)\n",
    "combined_logOR = np.sum(df[\"LogOR\"] * df[\"Weight\"]) / np.sum(df[\"Weight\"])\n",
    "combined_SE = np.sqrt(1 / np.sum(df[\"Weight\"]))  # SE of the combined effect\n",
    "\n",
    "# Calculate 95% CI for the combined effect\n",
    "z_score = stats.norm.ppf(0.975)  # 95% confidence level\n",
    "lower_CI_combined = np.exp(combined_logOR - z_score * combined_SE)\n",
    "upper_CI_combined = np.exp(combined_logOR + z_score * combined_SE)\n",
    "\n",
    "# Calculate I²\n",
    "Q = np.sum(df[\"Weight\"] * (df[\"LogOR\"] - combined_logOR) ** 2)  # Cochran's Q statistic\n",
    "df_ = len(df) - 1  # Degrees of freedom\n",
    "I2 = 100 * (Q - df_) / Q if Q > df_ else 0  # Calculate I²\n",
    "\n",
    "# Output results\n",
    "print(\"Fixed-effect meta-analysis result:\")\n",
    "print(f\"Combined OR: {np.exp(combined_logOR):.2f} [{lower_CI_combined:.2f}; {upper_CI_combined:.2f}]\")\n",
    "print(f\"I²: {I2:.2f}%\")\n",
    "\n",
    "# Calculate Q-statistic (Cochran's Q) and degrees of freedom\n",
    "Q = np.sum(df[\"Weight\"] * (df[\"LogOR\"] - combined_logOR) ** 2)\n",
    "df_ = len(df) - 1  # Degrees of freedom\n",
    "\n",
    "# Estimate between-study variance (tau^2) using DerSimonian-Laird method\n",
    "tau2 = max((Q - df_) / np.sum(df[\"Weight\"]), 0)\n",
    "\n",
    "# Adjust weights for the random-effects model\n",
    "df[\"Weight_random\"] = 1 / (df[\"SE\"]**2 + tau2)\n",
    "\n",
    "# Calculate the new pooled estimate using random-effects weights\n",
    "combined_logOR_random = np.sum(df[\"LogOR\"] * df[\"Weight_random\"]) / np.sum(df[\"Weight_random\"])\n",
    "combined_SE_random = np.sqrt(1 / np.sum(df[\"Weight_random\"]))  # SE of the combined effect for random-effects model\n",
    "\n",
    "# Calculate 95% CI for the combined effect using random-effects model\n",
    "lower_CI_combined_random = np.exp(combined_logOR_random - 1.96 * combined_SE_random)\n",
    "upper_CI_combined_random = np.exp(combined_logOR_random + 1.96 * combined_SE_random)\n",
    "\n",
    "# Calculate I² for random-effects model\n",
    "I2_random = 100 * (Q - df_) / Q if Q > df_ else 0\n",
    "\n",
    "# Output results for random-effects model\n",
    "print(\"\\nRandom-effects meta-analysis result:\")\n",
    "print(f\"Combined OR: {np.exp(combined_logOR_random):.2f} [{lower_CI_combined_random:.2f}; {upper_CI_combined_random:.2f}]\")\n",
    "print(f\"I²: {I2_random:.2f}%\")\n",
    "\n",
    "# Initialize lists to store results\n",
    "combined_ORs = []\n",
    "I2_values = []\n",
    "\n",
    "# Sensitivity Analysis: Exclude each study one by one\n",
    "print(\"\\nSensitivity Analysis:\")\n",
    "for index, study in df.iterrows():\n",
    "    df_sensitivity = df.drop(index)  # Remove one study at a time\n",
    "    \n",
    "    # Calculate LogOR and SE for the remaining studies\n",
    "    logOR_sensitivity = np.log(df_sensitivity[\"OR\"])\n",
    "    SE_sensitivity = (np.log(df_sensitivity[\"Upper CI\"]) - np.log(df_sensitivity[\"Lower CI\"])) / 3.92\n",
    "    weights_sensitivity = df_sensitivity[\"Sample Size\"] / (SE_sensitivity ** 2)\n",
    "\n",
    "    # Random effects meta-analysis for the remaining studies\n",
    "    combined_logOR_sensitivity = np.sum(logOR_sensitivity * weights_sensitivity) / np.sum(weights_sensitivity)\n",
    "    combined_SE_sensitivity = np.sqrt(1 / np.sum(weights_sensitivity))  # SE for the combined effect\n",
    "    Q_sensitivity = np.sum(weights_sensitivity * (logOR_sensitivity - combined_logOR_sensitivity) ** 2)\n",
    "    df_ = len(df_sensitivity) - 1  # Degrees of freedom for the remaining studies\n",
    "    tau2_sensitivity = max((Q_sensitivity - df_) / np.sum(weights_sensitivity), 0)\n",
    "    \n",
    "    # Recalculate weights for random effects\n",
    "    df_sensitivity[\"Weight_random\"] = 1 / (SE_sensitivity**2 + tau2_sensitivity)\n",
    "    \n",
    "    # Combined OR for random effects\n",
    "    combined_logOR_random_sensitivity = np.sum(logOR_sensitivity * df_sensitivity[\"Weight_random\"]) / np.sum(df_sensitivity[\"Weight_random\"])\n",
    "    combined_SE_random_sensitivity = np.sqrt(1 / np.sum(df_sensitivity[\"Weight_random\"]))  # SE for random effects\n",
    "    \n",
    "    # Confidence intervals for combined OR\n",
    "    lower_CI_combined_random_sensitivity = np.exp(combined_logOR_random_sensitivity - 1.96 * combined_SE_random_sensitivity)\n",
    "    upper_CI_combined_random_sensitivity = np.exp(combined_logOR_random_sensitivity + 1.96 * combined_SE_random_sensitivity)\n",
    "    \n",
    "    # Calculate I²\n",
    "    I2_sensitivity = 100 * (Q_sensitivity - df_) / Q_sensitivity if Q_sensitivity > df_ else 0\n",
    "    \n",
    "    # Append results for analysis\n",
    "    combined_ORs.append(np.exp(combined_logOR_random_sensitivity))\n",
    "    I2_values.append(I2_sensitivity)\n",
    "    \n",
    "    # Output the result for this iteration\n",
    "    print(f\"After excluding {study['Study']}:\")\n",
    "    print(f\"Combined OR: {np.exp(combined_logOR_random_sensitivity):.2f} [{lower_CI_combined_random_sensitivity:.2f}; {upper_CI_combined_random_sensitivity:.2f}]\")\n",
    "    print(f\"I²: {I2_sensitivity:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Compare the final combined OR and I² from all iterations\n",
    "print(\"\\nSummary of Sensitivity Analysis:\")\n",
    "print(f\"Combined ORs after excluding each study: {combined_ORs}\")\n",
    "print(f\"I² values after excluding each study: {I2_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "smoking            0      1\n",
      "dental caries              \n",
      "0              20207  10418\n",
      "1               4459   3900\n",
      "Odds Ratio (OR): 1.70\n",
      "95% Confidence Interval: [1.62, 1.78]\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/rishav/Downloads/archive/train_dataset.csv\")\n",
    "# Create a contingency table for dental caries and smoking\n",
    "contingency = pd.crosstab(df[\"dental caries\"], df[\"smoking\"])\n",
    "\n",
    "# Print the table\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency)\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Extract values from the contingency table\n",
    "a = contingency.loc[1, 1] if 1 in contingency.columns else 0  # Smokers with caries\n",
    "b = contingency.loc[1, 0] if 0 in contingency.columns else 0  # Non-smokers with caries\n",
    "c = contingency.loc[0, 1] if 1 in contingency.columns else 0  # Smokers without caries\n",
    "d = contingency.loc[0, 0] if 0 in contingency.columns else 0  # Non-smokers without caries\n",
    "\n",
    "# Calculate Odds Ratio\n",
    "odds_ratio = (a * d) / (b * c) if b * c > 0 else np.inf\n",
    "\n",
    "# Calculate the standard error of log(OR)\n",
    "if all(x > 0 for x in [a, b, c, d]):  # Check to avoid division by zero\n",
    "    se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "    log_or = np.log(odds_ratio)\n",
    "    ci_lower = np.exp(log_or - 1.96 * se_log_or)\n",
    "    ci_upper = np.exp(log_or + 1.96 * se_log_or)\n",
    "else:\n",
    "    ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "# Fisher's Exact Test for p-value\n",
    "_, p_value = fisher_exact([[a, b], [c, d]])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Odds Ratio (OR): {odds_ratio:.2f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"P-value: {p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "smoking            0      1\n",
      "dental caries              \n",
      "0              20207  10418\n",
      "1               4459   3900\n",
      "Odds Ratio (OR): 1.70\n",
      "95% Confidence Interval: [1.62, 1.78]\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/rishav/Downloads/archive/train_dataset.csv\")\n",
    "# Create a contingency table for dental caries and smoking\n",
    "contingency = pd.crosstab(df[\"dental caries\"], df[\"smoking\"])\n",
    "\n",
    "# Print the table\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency)\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Extract values from the contingency table\n",
    "a = contingency.loc[1, 1] if 1 in contingency.columns else 0  # Smokers with caries\n",
    "b = contingency.loc[1, 0] if 0 in contingency.columns else 0  # Non-smokers with caries\n",
    "c = contingency.loc[0, 1] if 1 in contingency.columns else 0  # Smokers without caries\n",
    "d = contingency.loc[0, 0] if 0 in contingency.columns else 0  # Non-smokers without caries\n",
    "\n",
    "# Calculate Odds Ratio\n",
    "odds_ratio = (a * d) / (b * c) if b * c > 0 else np.inf\n",
    "\n",
    "# Calculate the standard error of log(OR)\n",
    "if all(x > 0 for x in [a, b, c, d]):  # Check to avoid division by zero\n",
    "    se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "    log_or = np.log(odds_ratio)\n",
    "    ci_lower = np.exp(log_or - 1.96 * se_log_or)\n",
    "    ci_upper = np.exp(log_or + 1.96 * se_log_or)\n",
    "else:\n",
    "    ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "# Fisher's Exact Test for p-value\n",
    "_, p_value = fisher_exact([[a, b], [c, d]])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Odds Ratio (OR): {odds_ratio:.2f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"P-value: {p_value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
